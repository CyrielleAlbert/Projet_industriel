{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a24eb59c9a56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdepth_path\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'RGBD/depth/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimages_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#création des images Depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mimages_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'listdir' is not defined"
     ]
    }
   ],
   "source": [
    "rgb_path = 'RGBD/image/'\n",
    "depth_path ='RGBD/depth/'\n",
    "\n",
    "images_names = listdir(rgb_path)\n",
    "#création des images Depth\n",
    "for image_name in images_names:\n",
    "    img = cv2.imread(rgb_path+image_name)\n",
    "    RGB_image,depth_img = estimate_depth_from_RGB_image(img)\n",
    "    depth_img_pil = Image.fromarray(depth_img).convert(\"L\")\n",
    "    depth_img_pil.save(depth_path+image_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "from PIL import Image,ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des images Depth pour toute image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pose estimation\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "def pose_estimation(source_rgbd_image, target_rgbd_image,\n",
    "                    pinhole_camera_intrinsic, debug_draw_correspondences):\n",
    "    success = False\n",
    "    trans = np.identity(4)\n",
    "\n",
    "    # transform double array to unit8 array\n",
    "    color_cv_s = np.uint8(np.asarray(source_rgbd_image.color) * 255.0)\n",
    "    color_cv_t = np.uint8(np.asarray(target_rgbd_image.color) * 255.0)\n",
    "\n",
    "    orb = cv2.ORB_create(scaleFactor=1.2,\n",
    "                         nlevels=8,\n",
    "                         edgeThreshold=31,\n",
    "                         firstLevel=0,\n",
    "                         WTA_K=2,\n",
    "                         scoreType=cv2.ORB_HARRIS_SCORE,\n",
    "                         nfeatures=100,\n",
    "                         patchSize=31)  # to save time\n",
    "    [kp_s, des_s] = orb.detectAndCompute(color_cv_s, None)\n",
    "    [kp_t, des_t] = orb.detectAndCompute(color_cv_t, None)\n",
    "    if len(kp_s) == 0 or len(kp_t) == 0:\n",
    "        return success, trans\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(des_s, des_t)\n",
    "\n",
    "    pts_s = []\n",
    "    pts_t = []\n",
    "    for match in matches:\n",
    "        pts_t.append(kp_t[match.trainIdx].pt)\n",
    "        pts_s.append(kp_s[match.queryIdx].pt)\n",
    "    pts_s = np.asarray(pts_s)\n",
    "    pts_t = np.asarray(pts_t)\n",
    "    # inlier points after initial BF matching\n",
    "    if debug_draw_correspondences:\n",
    "        draw_correspondences(np.asarray(source_rgbd_image.color),\n",
    "                             np.asarray(target_rgbd_image.color), pts_s, pts_t,\n",
    "                             np.ones(pts_s.shape[0]), \"Initial BF matching\")\n",
    "\n",
    "    focal_input = (pinhole_camera_intrinsic.intrinsic_matrix[0, 0] +\n",
    "                   pinhole_camera_intrinsic.intrinsic_matrix[1, 1]) / 2.0\n",
    "    pp_x = pinhole_camera_intrinsic.intrinsic_matrix[0, 2]\n",
    "    pp_y = pinhole_camera_intrinsic.intrinsic_matrix[1, 2]\n",
    "\n",
    "    # Essential matrix is made for masking inliers\n",
    "    pts_s_int = np.int32(pts_s + 0.5)\n",
    "    pts_t_int = np.int32(pts_t + 0.5)\n",
    "    [E, mask] = cv2.findEssentialMat(pts_s_int,\n",
    "                                     pts_t_int,\n",
    "                                     focal=focal_input,\n",
    "                                     pp=(pp_x, pp_y),\n",
    "                                     method=cv2.RANSAC,\n",
    "                                     prob=0.999,\n",
    "                                     threshold=1.0)\n",
    "    if mask is None:\n",
    "        return success, trans\n",
    "\n",
    "    # inlier points after 5pt algorithm\n",
    "    if debug_draw_correspondences:\n",
    "        draw_correspondences(np.asarray(source_rgbd_image.color),\n",
    "                             np.asarray(target_rgbd_image.color), pts_s, pts_t,\n",
    "                             mask, \"5-pt RANSAC\")\n",
    "\n",
    "    # make 3D correspondences\n",
    "    depth_s = np.asarray(source_rgbd_image.depth)\n",
    "    depth_t = np.asarray(target_rgbd_image.depth)\n",
    "    pts_xyz_s = np.zeros([3, pts_s.shape[0]])\n",
    "    pts_xyz_t = np.zeros([3, pts_s.shape[0]])\n",
    "    cnt = 0\n",
    "    for i in range(pts_s.shape[0]):\n",
    "        if mask[i]:\n",
    "            xyz_s = get_xyz_from_pts(pts_s[i, :], depth_s, pp_x, pp_y,\n",
    "                                     focal_input)\n",
    "            pts_xyz_s[:, cnt] = xyz_s\n",
    "            xyz_t = get_xyz_from_pts(pts_t[i, :], depth_t, pp_x, pp_y,\n",
    "                                     focal_input)\n",
    "            pts_xyz_t[:, cnt] = xyz_t\n",
    "            cnt = cnt + 1\n",
    "    pts_xyz_s = pts_xyz_s[:, :cnt]\n",
    "    pts_xyz_t = pts_xyz_t[:, :cnt]\n",
    "\n",
    "    success, trans, inlier_id_vec = estimate_3D_transform_RANSAC(\n",
    "        pts_xyz_s, pts_xyz_t)\n",
    "\n",
    "    if debug_draw_correspondences:\n",
    "        pts_s_new = np.zeros(shape=(len(inlier_id_vec), 2))\n",
    "        pts_t_new = np.zeros(shape=(len(inlier_id_vec), 2))\n",
    "        mask = np.ones(len(inlier_id_vec))\n",
    "        cnt = 0\n",
    "        for i in inlier_id_vec:\n",
    "            u_s, v_s = get_uv_from_xyz(pts_xyz_s[0, i], pts_xyz_s[1, i],\n",
    "                                       pts_xyz_s[2, i], pp_x, pp_y, focal_input)\n",
    "            u_t, v_t = get_uv_from_xyz(pts_xyz_t[0, i], pts_xyz_t[1, i],\n",
    "                                       pts_xyz_t[2, i], pp_x, pp_y, focal_input)\n",
    "            pts_s_new[cnt, :] = [u_s, v_s]\n",
    "            pts_t_new[cnt, :] = [u_t, v_t]\n",
    "            cnt = cnt + 1\n",
    "        draw_correspondences(np.asarray(source_rgbd_image.color),\n",
    "                             np.asarray(target_rgbd_image.color), pts_s_new,\n",
    "                             pts_t_new, mask, \"5-pt RANSAC + 3D Rigid RANSAC\")\n",
    "    return success, trans\n",
    "\n",
    "\n",
    "def draw_correspondences(img_s, img_t, pts_s, pts_t, mask, title):\n",
    "    ha, wa = img_s.shape[:2]\n",
    "    hb, wb = img_t.shape[:2]\n",
    "    total_width = wa + wb\n",
    "    new_img = np.zeros(shape=(ha, total_width))\n",
    "    new_img[:ha, :wa] = img_s\n",
    "    new_img[:hb, wa:wa + wb] = img_t\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.canvas.set_window_title(title)\n",
    "    for i in range(pts_s.shape[0]):\n",
    "        if mask[i]:\n",
    "            sx = pts_s[i, 0]\n",
    "            sy = pts_s[i, 1]\n",
    "            tx = pts_t[i, 0] + wa\n",
    "            ty = pts_t[i, 1]\n",
    "            plt.plot([sx, tx], [sy, ty],\n",
    "                     color=np.random.random(3) / 2 + 0.5,\n",
    "                     lw=1.0)\n",
    "    plt.imshow(new_img)\n",
    "    plt.pause(0.5)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def estimate_3D_transform_RANSAC(pts_xyz_s, pts_xyz_t):\n",
    "    max_iter = 1000\n",
    "    max_distance = 0.05\n",
    "    n_sample = 5\n",
    "    n_points = pts_xyz_s.shape[1]\n",
    "    Transform_good = np.identity(4)\n",
    "    max_inlier = n_sample\n",
    "    inlier_vec_good = []\n",
    "    success = False\n",
    "\n",
    "    if n_points < n_sample:\n",
    "        return False, np.identity(4), []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        # sampling\n",
    "        rand_idx = np.random.randint(n_points, size=n_sample)\n",
    "        sample_xyz_s = pts_xyz_s[:, rand_idx]\n",
    "        sample_xyz_t = pts_xyz_t[:, rand_idx]\n",
    "        R_approx, t_approx = estimate_3D_transform(sample_xyz_s, sample_xyz_t)\n",
    "\n",
    "        # evaluation\n",
    "        diff_mat = pts_xyz_t - (np.matmul(R_approx, pts_xyz_s) +\n",
    "                                np.tile(t_approx, [1, n_points]))\n",
    "        diff = [np.linalg.norm(diff_mat[:, i]) for i in range(n_points)]\n",
    "        n_inlier = len([1 for diff_iter in diff if diff_iter < max_distance])\n",
    "\n",
    "        # note: diag(R_approx) > 0 prevents ankward transformation between\n",
    "        # RGBD pair of relatively small amount of baseline.\n",
    "        if (n_inlier > max_inlier) and (np.linalg.det(R_approx) != 0.0) and \\\n",
    "                (R_approx[0,0] > 0 and R_approx[1,1] > 0 and R_approx[2,2] > 0):\n",
    "            Transform_good[:3, :3] = R_approx\n",
    "            Transform_good[:3, 3] = [t_approx[0], t_approx[1], t_approx[2]]\n",
    "            max_inlier = n_inlier\n",
    "            inlier_vec = [id_iter for diff_iter, id_iter \\\n",
    "                    in zip(diff, range(n_points)) \\\n",
    "                    if diff_iter < max_distance]\n",
    "            inlier_vec_good = inlier_vec\n",
    "            success = True\n",
    "\n",
    "    return success, Transform_good, inlier_vec_good\n",
    "\n",
    "\n",
    "# singular value decomposition approach\n",
    "# based on the description in the sec 3.1.2 in\n",
    "# http://graphics.stanford.edu/~smr/ICP/comparison/eggert_comparison_mva97.pdf\n",
    "def estimate_3D_transform(input_xyz_s, input_xyz_t):\n",
    "    # compute H\n",
    "    xyz_s = copy.copy(input_xyz_s)\n",
    "    xyz_t = copy.copy(input_xyz_t)\n",
    "    n_points = xyz_s.shape[1]\n",
    "    mean_s = np.mean(xyz_s, axis=1)\n",
    "    mean_t = np.mean(xyz_t, axis=1)\n",
    "    mean_s.shape = (3, 1)\n",
    "    mean_t.shape = (3, 1)\n",
    "    xyz_diff_s = xyz_s - np.tile(mean_s, [1, n_points])\n",
    "    xyz_diff_t = xyz_t - np.tile(mean_t, [1, n_points])\n",
    "    H = np.matmul(xyz_diff_s, xyz_diff_t.transpose())\n",
    "    # solve system\n",
    "    U, s, V = np.linalg.svd(H)\n",
    "    R_approx = np.matmul(V.transpose(), U.transpose())\n",
    "    if np.linalg.det(R_approx) < 0.0:\n",
    "        det = np.linalg.det(np.matmul(U, V))\n",
    "        D = np.identity(3)\n",
    "        D[2, 2] = det\n",
    "        R_approx = np.matmul(U, np.matmul(D, V))\n",
    "    t_approx = mean_t - np.matmul(R_approx, mean_s)\n",
    "    return R_approx, t_approx\n",
    "\n",
    "\n",
    "def get_xyz_from_pts(pts_row, depth, px, py, focal):\n",
    "    u = pts_row[0]\n",
    "    v = pts_row[1]\n",
    "    u0 = int(u)\n",
    "    v0 = int(v)\n",
    "    height = depth.shape[0]\n",
    "    width = depth.shape[1]\n",
    "    # bilinear depth interpolation\n",
    "    if u0 > 0 and u0 < width - 1 and v0 > 0 and v0 < height - 1:\n",
    "        up = pts_row[0] - u0\n",
    "        vp = pts_row[1] - v0\n",
    "        d0 = depth[v0, u0]\n",
    "        d1 = depth[v0, u0 + 1]\n",
    "        d2 = depth[v0 + 1, u0]\n",
    "        d3 = depth[v0 + 1, u0 + 1]\n",
    "        d = (1 - vp) * (d1 * up + d0 * (1 - up)) + vp * (d3 * up + d2 *\n",
    "                                                         (1 - up))\n",
    "        return get_xyz_from_uv(u, v, d, px, py, focal)\n",
    "    else:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "\n",
    "def get_xyz_from_uv(u, v, d, px, py, focal):\n",
    "    if focal != 0:\n",
    "        x = (u - px) / focal * d\n",
    "        y = (v - py) / focal * d\n",
    "    else:\n",
    "        x = 0\n",
    "        y = 0\n",
    "    return np.array([x, y, d]).transpose()\n",
    "\n",
    "\n",
    "def get_uv_from_xyz(x, y, z, px, py, focal):\n",
    "    if z != 0:\n",
    "        u = focal * x / z + px\n",
    "        v = focal * y / z + py\n",
    "    else:\n",
    "        u = 0\n",
    "        v = 0\n",
    "    return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize_posegraph.py\n",
    "import sys\n",
    "sys.path.append(\"../utility\")\n",
    "\n",
    "def run_posegraph_optimization(pose_graph_name, pose_graph_optimized_name,\n",
    "                               max_correspondence_distance,\n",
    "                               preference_loop_closure):\n",
    "    # to display messages from o3d.pipelines.registration.global_optimization\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "    method = o3d.pipelines.registration.GlobalOptimizationLevenbergMarquardt()\n",
    "    criteria = o3d.pipelines.registration.GlobalOptimizationConvergenceCriteria(\n",
    "    )\n",
    "    option = o3d.pipelines.registration.GlobalOptimizationOption(\n",
    "        max_correspondence_distance=max_correspondence_distance,\n",
    "        edge_prune_threshold=0.25,\n",
    "        preference_loop_closure=preference_loop_closure,\n",
    "        reference_node=0)\n",
    "    pose_graph = o3d.io.read_pose_graph(pose_graph_name)\n",
    "    o3d.pipelines.registration.global_optimization(pose_graph, method, criteria,\n",
    "                                                   option)\n",
    "    o3d.io.write_pose_graph(pose_graph_optimized_name, pose_graph)\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "\n",
    "\n",
    "def optimize_posegraph_for_fragment(path_dataset, fragment_id, config):\n",
    "    pose_graph_name = join(path_dataset, \"fragments/fragment_{0}.json\".format(fragment_id))\n",
    "    pose_graph_optimized_name = join(\n",
    "        path_dataset,\"fragments/fragment_{0}_optimized.json\".format(fragment_id))\n",
    "    run_posegraph_optimization(pose_graph_name, pose_graph_optimized_name,\n",
    "            max_correspondence_distance = config[\"max_depth_diff\"],\n",
    "            preference_loop_closure = \\\n",
    "            config[\"preference_loop_closure_odometry\"])\n",
    "\n",
    "\n",
    "def optimize_posegraph_for_scene(path_dataset, config):\n",
    "    pose_graph_name = join(path_dataset, \"fragments/global_fragments.json\")\n",
    "    pose_graph_optimized_name = join(\n",
    "        path_dataset, \"fragments/global_fragments_optimized.json\")\n",
    "    run_posegraph_optimization(pose_graph_name, pose_graph_optimized_name,\n",
    "            max_correspondence_distance = config[\"voxel_size\"] * 1.4,\n",
    "            preference_loop_closure = \\\n",
    "            config[\"preference_loop_closure_registration\"])\n",
    "\n",
    "\n",
    "def optimize_posegraph_for_refined_scene(path_dataset, config):\n",
    "    pose_graph_name = join(path_dataset,\"fragments/refine_posegraph.json\")\n",
    "    pose_graph_optimized_name = join(\n",
    "        path_dataset, \"fragments/refine_posegraph_optimized.json\")\n",
    "    run_posegraph_optimization(pose_graph_name, pose_graph_optimized_name,\n",
    "            max_correspondence_distance = config[\"voxel_size\"] * 1.4,\n",
    "            preference_loop_closure = \\\n",
    "            config[\"preference_loop_closure_registration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV is detected. Using ORB + 5pt algorithm\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "import math\n",
    "\n",
    "sys.path.append(\".\")\n",
    "\n",
    "\n",
    "def initialize_opencv():\n",
    "    opencv_installed = True\n",
    "    try:\n",
    "        import cv2\n",
    "    except ImportError:\n",
    "        pass\n",
    "        print(\"OpenCV is not detected. Using Identity as an initial\")\n",
    "        opencv_installed = False\n",
    "    if opencv_installed:\n",
    "        print(\"OpenCV is detected. Using ORB + 5pt algorithm\")\n",
    "    return opencv_installed\n",
    "with_opencv=initialize_opencv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file.py\n",
    "from os import makedirs\n",
    "from os.path import exists, isfile, join, splitext, dirname, basename\n",
    "import shutil\n",
    "import re\n",
    "from warnings import warn\n",
    "import json\n",
    "\n",
    "\n",
    "def sorted_alphanum(file_list_ordered):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(file_list_ordered, key=alphanum_key)\n",
    "\n",
    "\n",
    "def get_file_list(path, extension=None):\n",
    "    if extension is None:\n",
    "        file_list = [path + f for f in listdir(path) if isfile(join(path, f))]\n",
    "    else:\n",
    "        file_list = [\n",
    "            path + f\n",
    "            for f in listdir(path)\n",
    "            if isfile(join(path, f)) and splitext(f)[1] == extension\n",
    "        ]\n",
    "    file_list = sorted_alphanum(file_list)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def add_if_exists(path_dataset, folder_names):\n",
    "    for folder_name in folder_names:\n",
    "        if exists(join(path_dataset, folder_name)):\n",
    "            path = join(path_dataset, folder_name)\n",
    "            return path\n",
    "    raise FileNotFoundError(\n",
    "        f\"None of the folders {folder_names} found in {path_dataset}\")\n",
    "\n",
    "\n",
    "def get_rgbd_folders(path_dataset):\n",
    "    path_color = add_if_exists(path_dataset, [\"image/\", \"rgb/\", \"color/\"])\n",
    "    path_depth = join(path_dataset, \"depth/\")\n",
    "    return path_color, path_depth\n",
    "\n",
    "\n",
    "def get_rgbd_file_lists(path_dataset):\n",
    "    path_color, path_depth = get_rgbd_folders(path_dataset)\n",
    "    color_files = get_file_list(path_color, \".jpg\") + \\\n",
    "            get_file_list(path_color, \".png\")\n",
    "    depth_files = get_file_list(path_depth, \".png\")\n",
    "    return color_files, depth_files\n",
    "\n",
    "\n",
    "def make_clean_folder(path_folder):\n",
    "    if not exists(path_folder):\n",
    "        makedirs(path_folder)\n",
    "    else:\n",
    "        shutil.rmtree(path_folder)\n",
    "        makedirs(path_folder)\n",
    "\n",
    "\n",
    "def check_folder_structure(path_dataset):\n",
    "    if isfile(path_dataset) and path_dataset.endswith(\".bag\"):\n",
    "        return\n",
    "    path_color, path_depth = get_rgbd_folders(path_dataset)\n",
    "    assert exists(path_depth), \\\n",
    "            \"Path %s is not exist!\" % path_depth\n",
    "    assert exists(path_color), \\\n",
    "            \"Path %s is not exist!\" % path_color\n",
    "\n",
    "\n",
    "def write_poses_to_log(filename, poses):\n",
    "    with open(filename, 'w') as f:\n",
    "        for i, pose in enumerate(poses):\n",
    "            f.write('{} {} {}\\n'.format(i, i, i + 1))\n",
    "            f.write('{0:.8f} {1:.8f} {2:.8f} {3:.8f}\\n'.format(\n",
    "                pose[0, 0], pose[0, 1], pose[0, 2], pose[0, 3]))\n",
    "            f.write('{0:.8f} {1:.8f} {2:.8f} {3:.8f}\\n'.format(\n",
    "                pose[1, 0], pose[1, 1], pose[1, 2], pose[1, 3]))\n",
    "            f.write('{0:.8f} {1:.8f} {2:.8f} {3:.8f}\\n'.format(\n",
    "                pose[2, 0], pose[2, 1], pose[2, 2], pose[2, 3]))\n",
    "            f.write('{0:.8f} {1:.8f} {2:.8f} {3:.8f}\\n'.format(\n",
    "                pose[3, 0], pose[3, 1], pose[3, 2], pose[3, 3]))\n",
    "\n",
    "\n",
    "def read_poses_from_log(traj_log):\n",
    "    import numpy as np\n",
    "\n",
    "    trans_arr = []\n",
    "    with open(traj_log) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "        # Load .log file.\n",
    "        for i in range(0, len(content), 5):\n",
    "            # format %d (src) %d (tgt) %f (fitness)\n",
    "            data = list(map(float, content[i].strip().split(' ')))\n",
    "            ids = (int(data[0]), int(data[1]))\n",
    "            fitness = data[2]\n",
    "\n",
    "            # format %f x 16\n",
    "            T_gt = np.array(\n",
    "                list(map(float, (''.join(\n",
    "                    content[i + 1:i + 5])).strip().split()))).reshape((4, 4))\n",
    "\n",
    "            trans_arr.append(T_gt)\n",
    "\n",
    "    return trans_arr\n",
    "\n",
    "\n",
    "def extract_rgbd_frames(rgbd_video_file):\n",
    "    \"\"\"\n",
    "    Extract color and aligned depth frames and intrinsic calibration from an\n",
    "    RGBD video file (currently only RealSense bag files supported). Folder\n",
    "    structure is:\n",
    "        <directory of rgbd_video_file/<rgbd_video_file name without extension>/\n",
    "            {depth/00000.jpg,color/00000.png,intrinsic.json}\n",
    "    \"\"\"\n",
    "    frames_folder = join(dirname(rgbd_video_file),\n",
    "                         basename(splitext(rgbd_video_file)[0]))\n",
    "    path_intrinsic = join(frames_folder, \"intrinsic.json\")\n",
    "    if isfile(path_intrinsic):\n",
    "        warn(f\"Skipping frame extraction for {rgbd_video_file} since files are\"\n",
    "             \" present.\")\n",
    "    else:\n",
    "        rgbd_video = o3d.t.io.RGBDVideoReader.create(rgbd_video_file)\n",
    "        rgbd_video.save_frames(frames_folder)\n",
    "    with open(path_intrinsic) as intr_file:\n",
    "        intr = json.load(intr_file)\n",
    "    depth_scale = intr[\"depth_scale\"]\n",
    "    return frames_folder, path_intrinsic, depth_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgbd_image(color_file, depth_file, convert_rgb_to_intensity, config):\n",
    "    color = o3d.io.read_image(color_file)\n",
    "    depth = o3d.io.read_image(depth_file)\n",
    "    rgbd_image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        color,\n",
    "        depth,\n",
    "        convert_rgb_to_intensity=convert_rgb_to_intensity)\n",
    "    return rgbd_image\n",
    "\n",
    "\n",
    "def register_one_rgbd_pair(s, t, color_files, depth_files, intrinsic,\n",
    "                           with_opencv, config):\n",
    "    source_rgbd_image = read_rgbd_image(color_files[s], depth_files[s], True,\n",
    "                                        config)\n",
    "    target_rgbd_image = read_rgbd_image(color_files[t], depth_files[t], True,\n",
    "                                        config)\n",
    "    print(\"hello\")\n",
    "    option = o3d.pipelines.odometry.OdometryOption()\n",
    "    option.max_depth_diff = config[\"max_depth_diff\"]\n",
    "    if abs(s - t) != 1:\n",
    "        if with_opencv:\n",
    "            print(\"1\")\n",
    "            success_5pt, odo_init = pose_estimation(source_rgbd_image,\n",
    "                                                    target_rgbd_image,\n",
    "                                                    intrinsic, False)\n",
    "            if success_5pt:\n",
    "                print('2')\n",
    "                [success, trans, info\n",
    "                ] = o3d.pipelines.odometry.compute_rgbd_odometry(\n",
    "                    source_rgbd_image, target_rgbd_image, intrinsic, odo_init,\n",
    "                    o3d.pipelines.odometry.RGBDOdometryJacobianFromHybridTerm(),\n",
    "                    option)\n",
    "                return [success, trans, info]\n",
    "        print(\"3\")\n",
    "        return [False, np.identity(4), np.identity(6)]\n",
    "    else:\n",
    "        print(\"4\")\n",
    "        odo_init = np.identity(4)\n",
    "        [success, trans, info] = o3d.pipelines.odometry.compute_rgbd_odometry(\n",
    "            source_rgbd_image, target_rgbd_image, intrinsic, odo_init,\n",
    "            o3d.pipelines.odometry.RGBDOdometryJacobianFromHybridTerm(), option)\n",
    "        print(\"hi\")\n",
    "        return [success, trans, info]\n",
    "\n",
    "\n",
    "def make_posegraph_for_fragment(path_dataset, sid, eid, color_files,\n",
    "                                depth_files, fragment_id, n_fragments,\n",
    "                                intrinsic, with_opencv, config):\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "    pose_graph = o3d.pipelines.registration.PoseGraph()\n",
    "    trans_odometry = np.identity(4)\n",
    "    pose_graph.nodes.append(\n",
    "        o3d.pipelines.registration.PoseGraphNode(trans_odometry))\n",
    "    for s in range(sid, eid):\n",
    "        for t in range(s + 1, eid):\n",
    "            # odometry\n",
    "            if t == s + 1:\n",
    "                print(\n",
    "                    \"Fragment %03d / %03d :: RGBD matching between frame : %d and %d\"\n",
    "                    % (fragment_id, n_fragments - 1, s, t))\n",
    "                [success, trans,\n",
    "                 info] = register_one_rgbd_pair(s, t, color_files, depth_files,\n",
    "                                                intrinsic, with_opencv, config)\n",
    "                trans_odometry = np.dot(trans, trans_odometry)\n",
    "                trans_odometry_inv = np.linalg.inv(trans_odometry)\n",
    "                pose_graph.nodes.append(\n",
    "                    o3d.pipelines.registration.PoseGraphNode(\n",
    "                        trans_odometry_inv))\n",
    "                pose_graph.edges.append(\n",
    "                    o3d.pipelines.registration.PoseGraphEdge(s - sid,\n",
    "                                                             t - sid,\n",
    "                                                             trans,\n",
    "                                                             info,\n",
    "                                                             uncertain=False))\n",
    "\n",
    "            # keyframe loop closure\n",
    "            if s % config['n_keyframes_per_n_frame'] == 0 \\\n",
    "                    and t % config['n_keyframes_per_n_frame'] == 0:\n",
    "                print(\n",
    "                    \"Fragment %03d / %03d :: RGBD matching between 2 frame : %d and %d\"\n",
    "                    % (fragment_id, n_fragments - 1, s, t))\n",
    "                [success, trans,\n",
    "                 info] = register_one_rgbd_pair(s, t, color_files, depth_files,\n",
    "                                                intrinsic, with_opencv, config)\n",
    "                if success:\n",
    "                    pose_graph.edges.append(\n",
    "                        o3d.pipelines.registration.PoseGraphEdge(\n",
    "                            s - sid, t - sid, trans, info, uncertain=True))\n",
    "    o3d.io.write_pose_graph(\n",
    "        join(path_dataset, \"fragments/fragments_posegraph_{0}.json\".format(fragment_id)),\n",
    "        pose_graph)\n",
    "\n",
    "\n",
    "def integrate_rgb_frames_for_fragment(color_files, depth_files, fragment_id,\n",
    "                                      n_fragments, pose_graph_name, intrinsic,\n",
    "                                      config):\n",
    "    pose_graph = o3d.io.read_pose_graph(pose_graph_name)\n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length=config[\"tsdf_cubic_size\"] / 512.0,\n",
    "        sdf_trunc=0.04,\n",
    "        color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)\n",
    "    for i in range(len(pose_graph.nodes)):\n",
    "        i_abs = fragment_id * config['n_frames_per_fragment'] + i\n",
    "        print(\n",
    "            \"Fragment %03d / %03d :: integrate rgbd frame %d (%d of %d).\" %\n",
    "            (fragment_id, n_fragments - 1, i_abs, i + 1, len(pose_graph.nodes)))\n",
    "        rgbd = read_rgbd_image(color_files[i_abs], depth_files[i_abs], False,\n",
    "                               config)\n",
    "        pose = pose_graph.nodes[i].pose\n",
    "        volume.integrate(rgbd, intrinsic, np.linalg.inv(pose))\n",
    "    mesh = volume.extract_triangle_mesh()\n",
    "    mesh.compute_vertex_normals()\n",
    "    return mesh\n",
    "\n",
    "\n",
    "def make_pointcloud_for_fragment(path_dataset, color_files, depth_files,\n",
    "                                 fragment_id, n_fragments, intrinsic, config):\n",
    "    print(\"hello\")\n",
    "    mesh = integrate_rgb_frames_for_fragment(\n",
    "        color_files, depth_files, fragment_id, n_fragments,\n",
    "        join(path_dataset,\n",
    "             \"fragments/fragment_posegraph_{0}_optimized.json\".format(fragment_id)),\n",
    "        intrinsic, config)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = mesh.vertices\n",
    "    pcd.colors = mesh.vertex_colors\n",
    "    pcd_name = join(path_dataset,\n",
    "                    \"fragments/fragment_poincloud_{0}.ply\".format(fragment_id))\n",
    "    print(pcd_name)\n",
    "    o3d.io.write_point_cloud(pcd_name, pcd, False, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_fragment(fragment_id, color_files, depth_files, n_files,\n",
    "                            n_fragments, config):\n",
    "    if config[\"path_intrinsic\"]:\n",
    "        intrinsic = o3d.io.read_pinhole_camera_intrinsic(\n",
    "            config[\"path_intrinsic\"])\n",
    "    else:\n",
    "        intrinsic = o3d.camera.PinholeCameraIntrinsic(width=960,height=1280,cx=480,cy=640,fx=18564,fy=18564)\n",
    "    sid = fragment_id * config['n_frames_per_fragment']\n",
    "    eid = min(sid + config['n_frames_per_fragment'], n_files)\n",
    "\n",
    "    make_posegraph_for_fragment(config[\"path_dataset\"], sid, eid, color_files,\n",
    "                                depth_files, fragment_id, n_fragments,\n",
    "                                intrinsic, with_opencv, config)\n",
    "    optimize_posegraph_for_fragment(config[\"path_dataset\"], fragment_id, config)\n",
    "    make_pointcloud_for_fragment(config[\"path_dataset\"], color_files,\n",
    "                                 depth_files, fragment_id, n_fragments,\n",
    "                                 intrinsic, config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clean_folder(path_folder):\n",
    "    if not exists(path_folder):\n",
    "        makedirs(path_folder)\n",
    "    else:\n",
    "        user_input = input(\"%s not empty. Overwrite? (y/n) : \" % path_folder)\n",
    "        if user_input.lower() == 'y':\n",
    "            shutil.rmtree(path_folder)\n",
    "            makedirs(path_folder)\n",
    "        else:\n",
    "            exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config):\n",
    "    print(\"making fragments from RGBD sequence.\")\n",
    "    make_clean_folder(join(config[\"path_dataset\"], config[\"folder_fragment\"]))\n",
    "\n",
    "    color_files = [rgb_path+ img_name for img_name in images_names]\n",
    "    depth_files = [depth_path+ img_name for img_name in images_names]\n",
    "\n",
    "    n_files = len(color_files)\n",
    "    n_fragments = int(\n",
    "        math.ceil(float(n_files) / config['n_frames_per_fragment']))\n",
    "\n",
    "    if config[\"python_multi_threading\"] is True:\n",
    "        from joblib import Parallel, delayed\n",
    "        import multiprocessing\n",
    "        import subprocess\n",
    "        MAX_THREAD = min(multiprocessing.cpu_count(), n_fragments)\n",
    "        Parallel(n_jobs=MAX_THREAD)(delayed(process_single_fragment)(\n",
    "            fragment_id, color_files, depth_files, n_files, n_fragments, config)\n",
    "                                    for fragment_id in range(n_fragments))\n",
    "    else:\n",
    "        for fragment_id in range(n_fragments):\n",
    "            process_single_fragment(fragment_id, color_files, depth_files,\n",
    "                                    n_files, n_fragments, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making fragments from RGBD sequence.\n",
      "../RGBD/fragments not empty. Overwrite? (y/n) : y\n",
      "Fragment 000 / 002 :: RGBD matching between frame : 0 and 1\n",
      "hello\n",
      "4\n",
      "hi\n",
      "Fragment 000 / 002 :: RGBD matching between 2 frame : 0 and 2\n",
      "hello\n",
      "1\n",
      "2\n",
      "Fragment 000 / 002 :: RGBD matching between 2 frame : 0 and 4\n",
      "hello\n",
      "1\n",
      "2\n",
      "Fragment 000 / 002 :: RGBD matching between frame : 1 and 2\n",
      "hello\n",
      "4\n",
      "hi\n",
      "Fragment 000 / 002 :: RGBD matching between frame : 2 and 3\n",
      "hello\n",
      "4\n",
      "hi\n",
      "Fragment 000 / 002 :: RGBD matching between 2 frame : 2 and 4\n",
      "hello\n",
      "1\n",
      "2\n",
      "Fragment 000 / 002 :: RGBD matching between frame : 3 and 4\n",
      "hello\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "config_path = 'config/classe.json'\n",
    "rgb_path = \"RGBD/image/\"\n",
    "depth_path = \"RGBD/depth/\"\n",
    "images_names = listdir(rgb_path)\n",
    "\n",
    "with open(config_path,'r') as file:\n",
    "    config = json.load(file)\n",
    "    \n",
    "run(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
